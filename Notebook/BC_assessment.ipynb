{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "J-7FeDyjxAnw",
        "UTnE6F1uDcK4",
        "HRoLCpOzDrgw",
        "zbIRW6DN1UYg",
        "ZNbGITdWF9AQ",
        "8LSIyaaRg7xm",
        "EW7buQdt-ZvK",
        "3QDWO4QeEr_F",
        "NZrMqtEqR_9I",
        "6yhWEHAKd-nA",
        "QSCBM3XYCvjR",
        "zxTWt7Z7MCL7",
        "GWt7iKqufdo2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Required packages and libraries"
      ],
      "metadata": {
        "id": "J-7FeDyjxAnw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkBJuk_EvtGv"
      },
      "outputs": [],
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install requests\n",
        "!pip install urllib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-QmY2tO1wGvN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4668c9f-8ced-472e-da22-24447620939e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import re\n",
        "import pandas as pd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwxMPIGSv_Gh",
        "outputId": "433a491f-c943-4ead-cea7-9605a594c28b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Extraction"
      ],
      "metadata": {
        "id": "UTnE6F1uDcK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "run:\n",
        "!python text_extraction.py -h for help"
      ],
      "metadata": {
        "id": "eiQr10YuIfnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/Text_EA/Code/text_extraction.py -p /content/drive/MyDrive/Text_EA/Input.xlsx -d /content/drive/MyDrive/Text_EA/extracted_data"
      ],
      "metadata": {
        "id": "9lqLAzIAZJeN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c137cc0d-dc3d-493d-cca3-b6048fd54cd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting...:   6% 7/114 [00:07<01:51,  1.04s/it]\n",
            "Maybe the site with url_id: 44.0 and url: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/ is empty or broken\n",
            "Extracting...:  18% 20/114 [00:20<01:35,  1.01s/it]\n",
            "Maybe the site with url_id: 57.0 and url: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/ is empty or broken\n",
            "Extracting...:  94% 107/114 [01:47<00:06,  1.01it/s]\n",
            "Maybe the site with url_id: 144.0 and url: https://insights.blackcoffer.com/ensuring-growth-through-insurance-technology/ is empty or broken\n",
            "Extracting...: 100% 114/114 [01:54<00:00,  1.00s/it]\n",
            "\n",
            "Extraction done from all the given links and the text files have been saved at /content/drive/MyDrive/Text_EA/extracted_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting url_ids of not working sites and deleting their row in the excel file"
      ],
      "metadata": {
        "id": "d5lW0vc4qVTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting working url ids\n",
        "\n",
        "file1 = open('/content/drive/MyDrive/Text_EA/working_url_id.json', 'r')\n",
        "working_urls = json.load(file1)\n",
        "\n",
        "#getting not working url id list\n",
        "\n",
        "not_working_url_id = []\n",
        "for url_id in list(range(37, 151)):\n",
        "  if url_id not in working_urls:\n",
        "    not_working_url_id.append(url_id)\n",
        "  else:\n",
        "    continue"
      ],
      "metadata": {
        "id": "bp0140yQnFdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('/content/drive/MyDrive/Text_EA/Output Data Structure.xlsx')"
      ],
      "metadata": {
        "id": "HKPlPZHmrxO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.set_index(['URL_ID'], inplace = True)"
      ],
      "metadata": {
        "id": "NsyP7azjs2eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(not_working_url_id, axis = 0,inplace = True)\n",
        "display(df)"
      ],
      "metadata": {
        "id": "AZAhu1aytFPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_excel('/content/drive/MyDrive/Text_EA/new Output Data Structure.xlsx')"
      ],
      "metadata": {
        "id": "qdLy0ppttHXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Analysis"
      ],
      "metadata": {
        "id": "HRoLCpOzDrgw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing stopwords"
      ],
      "metadata": {
        "id": "zbIRW6DN1UYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/bc_assessment/Code')"
      ],
      "metadata": {
        "id": "L2eysgUM3kC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python stopwords_remover.py -s /content/drive/MyDrive/Text_EA/stopwords -e /content/drive/MyDrive/Text_EA/extracted_data -d /content/drive/MyDrive/Text_EA/filtered_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1cc6kJS7AR0",
        "outputId": "19c5db27-b255-4381-cfcc-6ec86805b825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----Storing all stopwords file in a single file----\n",
            "\n",
            "\rLoading:   0% 0/9 [00:00<?, ?it/s]\rLoading: 100% 9/9 [00:00<00:00, 362.21it/s]\n",
            "\n",
            "----Filtering the extracted files----\n",
            "\n",
            "Loading: 100% 111/111 [00:08<00:00, 13.11it/s]\n",
            "\n",
            "----Text files are filtered and stored at /content/drive/MyDrive/Text_EA/filtered_data----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating positive and negative words dictionary"
      ],
      "metadata": {
        "id": "ZNbGITdWF9AQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_values_in_dict(word_dict, key, list_of_words):\n",
        "    ''' Append multiple values to a key in \n",
        "        the given dictionary '''\n",
        "    if key not in word_dict:\n",
        "        word_dict[key] = list()\n",
        "    word_dict[key].append(list_of_words)\n",
        "    return word_dict"
      ],
      "metadata": {
        "id": "M2iafGw0F8G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating dictionary\n",
        "\n",
        "word_dict = {'Positive':[], 'Negative':[]}"
      ],
      "metadata": {
        "id": "UNSEjFYrKJa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/Text_EA/MasterDictionary'\n",
        "path_filtered = '/content/drive/MyDrive/Text_EA/filtered_data'"
      ],
      "metadata": {
        "id": "gdOLVRoaKWH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting positive and negative words lists\n",
        "\n",
        "positive = []\n",
        "negative = []\n",
        "for filename in os.listdir(path):\n",
        "  f = os.path.join(path, filename)\n",
        "  with open(f, 'r', encoding='latin-1') as file1:\n",
        "    if filename == 'positive-words.txt':\n",
        "      positive = (file1.read().split())\n",
        "    elif filename == 'negative-words.txt':\n",
        "      negative = (file1.read().split())\n",
        "    else:\n",
        "      continue"
      ],
      "metadata": {
        "id": "klnJ5GF3KZMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adding positive and negative words to the dictionary\n",
        "\n",
        "for filename in tqdm(os.listdir(path_filtered)):\n",
        "  f = os.path.join(path_filtered, filename)\n",
        "  if os.path.isfile(f):\n",
        "    with open(f, 'r') as file1:\n",
        "      words = file1.read().split()\n",
        "      for r in words:\n",
        "        if r in positive:\n",
        "          word_dict = add_values_in_dict(word_dict, 'Positive', r)\n",
        "        elif r in negative:\n",
        "          word_dict = add_values_in_dict(word_dict, 'Negative', r)\n",
        "        else:\n",
        "          continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bpJqiQYM24w",
        "outputId": "e6ad48ff-3123-4f5e-bb42-391099ec9a91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 111/111 [00:07<00:00, 13.91it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file1 = open('/content/drive/MyDrive/Text_EA/dictionary/dictionary.json', 'w')\n",
        "json.dump(word_dict, file1)\n",
        "file1.close()"
      ],
      "metadata": {
        "id": "tCY19Az0_kim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word and Sentence tokenization"
      ],
      "metadata": {
        "id": "8LSIyaaRg7xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_filtered = '/content/drive/MyDrive/Text_EA/filtered_data'\n",
        "path_dest = '/content/drive/MyDrive/Text_EA/tokens'"
      ],
      "metadata": {
        "id": "9looujQy5Kr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "for filename in tqdm(os.listdir(path_filtered)):\n",
        "  f = os.path.join(path_filtered, filename)\n",
        "  with open(f, 'r') as file1:\n",
        "    text = file1.read()\n",
        "    file1.close()\n",
        "  base, ext = os.path.splitext(filename)\n",
        "  word_tokens = word_tokenize(text)\n",
        "  sent_tokens = sent_tokenize(text)\n",
        "  \n",
        "  word_destination = path_dest + '/word_tokens/wt_' + base + '.json'\n",
        "  sent_destination = path_dest + '/sent_tokens/st_' + base + '.json'\n",
        "  \n",
        "  with open(word_destination, 'w') as a:\n",
        "    json.dump(word_tokens, a)\n",
        "  a.close()\n",
        "  with open(sent_destination,'w') as a:\n",
        "    json.dump(sent_tokens, a)\n",
        "  a.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZA4r3ZLPHtb",
        "outputId": "b0c4c42b-d3cb-4845-d1bc-2dd4be6f132f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 111/111 [01:05<00:00,  1.69it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting derived variables:\n",
        "**Positive Score**: This score is calculated by assigning the value of +1 for each word if found in the Positive Dictionary and then adding up all the values.\n",
        "\n",
        "\n",
        "**Negative Score**: This score is calculated by assigning the value of -1 for each word if found in the Negative Dictionary and then adding up all the values. We multiply the score with -1 so that the score is a positive number.\n",
        "\n",
        "\n",
        "**Polarity Score**: This is the score that determines if a given text is positive or negative in nature. It is calculated by using the formula:\n",
        "\n",
        "Polarity Score = (Positive Score – Negative Score)/ ((Positive Score + Negative Score) + 0.000001)\n",
        "Range is from -1 to +1\n",
        "\n",
        "\n",
        "**Subjectivity Score**: This is the score that determines if a given text is objective or subjective. It is calculated by using the formula:\n",
        "\n",
        "Subjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)\n",
        "Range is from 0 to +1\n"
      ],
      "metadata": {
        "id": "EW7buQdt-ZvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_dict = '/content/drive/MyDrive/Text_EA/dictionary/dictionary.json'"
      ],
      "metadata": {
        "id": "i0ANTtpkBsaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting positive and negative dictionary\n",
        "# reading the file will return a text file,\n",
        "# so load it\n",
        "\n",
        "dictionary = open(path_dict, 'r')\n",
        "word_dict = json.load(dictionary)\n",
        "dictionary.close()"
      ],
      "metadata": {
        "id": "fup5w3Xy-Yux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_wt = '/content/drive/MyDrive/Text_EA/tokens/word_tokens'"
      ],
      "metadata": {
        "id": "c-OCA_bPJT-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_score = []\n",
        "n_score = []\n",
        "polarity_score = []\n",
        "subjectivity_score = []\n",
        "url_ids = []\n",
        "\n",
        "\n",
        "for filename in tqdm(os.listdir(path_wt)):\n",
        "  f = os.path.join(path_wt, filename)\n",
        "  file1 = open(f, 'r')\n",
        "  words = json.load(file1)\n",
        "  p = 0\n",
        "  n = 0\n",
        "  polar_score = 0\n",
        "  no_of_words = len(words)\n",
        "#  print(no_of_words, filename)\n",
        "  for word in words:\n",
        "    if word in word_dict['Positive']:\n",
        "      p += 1\n",
        "    elif word in word_dict['Negative']:\n",
        "      n += 1\n",
        "    else:\n",
        "      continue\n",
        "  #print('p',(p - n)/((p + n) + 0.000001))\n",
        "  url_ids.append(re.findall(\"\\d+\\.\\d+\", filename))\n",
        "  flat_list = []\n",
        "  p_score.append(p)\n",
        "  n_score.append(n)\n",
        "  polarity_score.append((p - n)/((p + n) + 0.000001))\n",
        "  subjectivity_score.append((p + n)/((no_of_words) + 0.000001))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6RLQOK5CTfo",
        "outputId": "af0b2673-158d-4b51-d1e0-c22ba9c73d43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 111/111 [00:06<00:00, 16.35it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def flat_list(list_2d):\n",
        "  flat_list = []\n",
        "  for sublist in list_2d:\n",
        "      for item in sublist:\n",
        "        flat_list.append(item)\n",
        "  return flat_list\n",
        "\n",
        "flat_list1 = flat_list(url_ids)"
      ],
      "metadata": {
        "id": "MIEvpzqpQ55J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = {'url_id': flat_list, 'p_score': p_score, 'n_score': n_score, \n",
        "         'polarity_score': polarity_score,\n",
        "         'subjectivity_score': subjectivity_score}\n",
        "file1 = open('/content/drive/MyDrive/Text_EA/result/scores.json', 'w')\n",
        "json.dump(score, file1)\n",
        "file1.close()"
      ],
      "metadata": {
        "id": "PaTnw2bAJ_6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis of Readability\n",
        "\n",
        "It is calculated using the Gunning Fox index formula described below:\n",
        "\n",
        "Average Sentence Length = the number of words / the number of sentences\n",
        "\n",
        "Percentage of Complex words = the number of complex words / the number of words\n",
        "\n",
        "Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)"
      ],
      "metadata": {
        "id": "3QDWO4QeEr_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_path = '/content/drive/MyDrive/Text_EA/tokens/word_tokens'\n",
        "sent_path = '/content/drive/MyDrive/Text_EA/tokens/sent_tokens'"
      ],
      "metadata": {
        "id": "tcNgrw5EF7OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = [os.listdir(word_path), os.listdir(sent_path)]"
      ],
      "metadata": {
        "id": "QmyocH-LBTCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_complex_words(words_list):\n",
        "  c = 0\n",
        "  for word in words_list:\n",
        "    l = re.findall('(?!e$)[aeiou]+', word, re.I)+re.findall('^[aeiouy]*e$', word, re.I)\n",
        "    if len(l) > 2:\n",
        "      c += 1\n",
        "  return c"
      ],
      "metadata": {
        "id": "ALSu2iAAGCKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_sent_length = []\n",
        "percent_of_complex_words = []\n",
        "fog_index = []\n",
        "url_id = []\n",
        "\n",
        "for i in tqdm(range(len(files[0]))):\n",
        "  w = os.path.join(word_path, files[0][i])\n",
        "  s = os.path.join(sent_path, files[1][i])\n",
        "\n",
        "  file1 = open(w, 'r')\n",
        "  words = json.load(file1)\n",
        "  file1.close()\n",
        "  no_of_words = len(words)\n",
        "\n",
        "  file2 = open(s, 'r')\n",
        "  sent = json.load(file2)\n",
        "  file2.close()\n",
        "\n",
        "  url_id.append(re.findall(\"\\d+\\.\\d+\", files[0][i]))\n",
        "  avg_sent_length.append(int(no_of_words/len(sent)))\n",
        "  percent_of_complex_words.append(count_complex_words(words)/no_of_words)\n",
        "  fog_index.append(0.4*(avg_sent_length[i]+percent_of_complex_words[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0GkrpWZB0Ra",
        "outputId": "50a570e6-2edb-478f-992f-451830e97323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 111/111 [00:00<00:00, 169.43it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flat_list1 = flat_list(url_id)"
      ],
      "metadata": {
        "id": "I72JF7JCQ2un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "readability = {'url_id': flat_list, 'avg_sent_length': avg_sent_length,\n",
        "               'percent_of_complex_words': percent_of_complex_words,\n",
        "               'fog_index': fog_index}"
      ],
      "metadata": {
        "id": "vf0dNKbQCvWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file1 = open('/content/drive/MyDrive/Text_EA/result/readability_analysis.json', 'w')\n",
        "json.dump(readability, file1)\n",
        "file1.close()"
      ],
      "metadata": {
        "id": "BCjpJdPZKjFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Average Number of Words Per Sentence\n",
        "The formula for calculating is:\n",
        "\n",
        "Average Number of Words Per Sentence = the total number of words / the total number of sentences\n",
        "\n",
        "###Complex Word Count\n",
        "Complex words are words in the text that contain more than two syllables.\n"
      ],
      "metadata": {
        "id": "NZrMqtEqR_9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_extracted = '/content/drive/MyDrive/Text_EA/extracted_data'"
      ],
      "metadata": {
        "id": "hOMDDL00f14U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "avg_no_words_per_sent = []\n",
        "complex_word_count = []\n",
        "url_id = []\n",
        "\n",
        "for filename in tqdm(os.listdir(path_extracted)):\n",
        "  f = os.path.join(path_extracted, filename)\n",
        "  with open(f, 'r') as file1:\n",
        "    text = file1.read()\n",
        "    file1.close()\n",
        "    base, ext = os.path.splitext(filename)\n",
        "    word_tokens = word_tokenize(text)\n",
        "    sent_tokens = sent_tokenize(text)\n",
        "\n",
        "    url_id.append(base)\n",
        "    avg_no_words_per_sent.append(int(len(word_tokens)/len(sent_tokens)))\n",
        "    complex_word_count.append(count_complex_words(word_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rfxwa6ygf18B",
        "outputId": "7387cbdc-5697-4e59-b461-6a708588958c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 111/111 [00:01<00:00, 56.60it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_words_and_complex_words = {'url_id': url_id, 'avg_no_words_per_sent': avg_no_words_per_sent,\n",
        "                               'compplex_word_count': complex_word_count}"
      ],
      "metadata": {
        "id": "J1yp0oXFLlvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file1 = open('/content/drive/MyDrive/Text_EA/result/avg_and_complex_words.json', 'w')\n",
        "json.dump(avg_words_and_complex_words, file1)\n",
        "file1.close()"
      ],
      "metadata": {
        "id": "s1UeOPtoa-Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Count\n",
        "We count the total cleaned words present in the text by \n",
        "1. removing the stop words (using stopwords class of nltk package).\n",
        "2. removing any punctuations like ? ! , . from the word before counting.\n",
        "\n",
        "### Syllable Count Per Word\n",
        "We count the number of Syllables in each word of the text by counting the vowels present in each word. We also handle some exceptions like words ending with \"es\",\"ed\" by not counting them as a syllable.\n"
      ],
      "metadata": {
        "id": "6yhWEHAKd-nA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_syllables(word):\n",
        "  c = 0\n",
        "  vowels = 'aeiou'\n",
        "  l = re.findall(f'(?!e$)(?!es$)(?!ed$)[{vowels}]', word, re.I)\n",
        "  return len(l)"
      ],
      "metadata": {
        "id": "HieD7otfynv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# using RegexpTokenizer to remove punctuations\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "url_id = []\n",
        "word_count = []\n",
        "syllable_count = []\n",
        "\n",
        "for filename in tqdm(os.listdir(path_extracted)):\n",
        "  f = os.path.join(path_extracted, filename)\n",
        "  base, ext = os.path.splitext(filename)\n",
        "  with open(f, 'r') as file1:\n",
        "    text = file1.read()\n",
        "    file1.close()\n",
        "  \n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  words = tokenizer.tokenize(text)\n",
        "  filtered_words = []\n",
        "  syllable_per_word = []\n",
        "  for word in words:\n",
        "    if word not in stop_words:\n",
        "          filtered_words.append(word)\n",
        "          list1 = []\n",
        "          list1.append(word)\n",
        "          list1.append(count_syllables(word))\n",
        "          syllable_per_word.append(list1)\n",
        "\n",
        "  path = '/content/drive/MyDrive/Text_EA/result/syllable_per_word' + '/' + base + '_syllable_count_per_word.json'\n",
        "  file1 = open(path, 'w')\n",
        "  json.dump(syllable_per_word, file1)\n",
        "  file1.close()\n",
        "\n",
        "  url_id.append(base)\n",
        "  word_count.append(len(filtered_words))\n",
        "  syllable_sum = 0\n",
        "  for i in range(len(syllable_per_word)):\n",
        "    syllable_sum += syllable_per_word[i][1]\n",
        "  syllable_count.append(int(syllable_sum/len(syllable_per_word)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cydQSDdd9WS",
        "outputId": "389ba65d-4409-4ec1-cb38-2e38cca327e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 111/111 [00:01<00:00, 66.16it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_and_syllable_count = {'url_id': url_id, 'word_count': word_count, 'syllable_count': syllable_count}"
      ],
      "metadata": {
        "id": "BG8HcPQ2nN4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file1 = open('/content/drive/MyDrive/Text_EA/result/word_and_syllable_count.json','w')\n",
        "json.dump(word_and_syllable_count, file1)\n",
        "file1.close()"
      ],
      "metadata": {
        "id": "kOaabqo89Ocw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Personal Pronouns\n",
        "To calculate Personal Pronouns mentioned in the text, we use regex to find the counts of the words - “I,” “we,” “my,” “ours,” and “us”. Special care is taken so that the country name US is not included in the list.\n",
        "\n",
        "###Average Word Length\n",
        "Average Word Length is calculated by the formula:\n",
        "Sum of the total number of characters in each word/Total number of words\n"
      ],
      "metadata": {
        "id": "QSCBM3XYCvjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_personal_pronouns(text):\n",
        "  pronoun_count = re.compile(r'\\b(I|we|ours|my|mine|(?-i:us))\\b', re.I)\n",
        "  pronouns = pronoun_count.findall(text)\n",
        "  return len(pronouns)"
      ],
      "metadata": {
        "id": "uaxtFZD3CF25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url_id = []\n",
        "personal_pronouns_count = []\n",
        "word_avg_length = []\n",
        "\n",
        "for filename in tqdm(os.listdir(path_extracted)):\n",
        "  f = os.path.join(path_extracted, filename)\n",
        "  base, ext = os.path.splitext(filename)\n",
        "  with open(f, 'r') as file1:\n",
        "    text = file1.read()\n",
        "    words = text.split()\n",
        "    file1.close()\n",
        "  \n",
        "  c = 0\n",
        "  for word in words:\n",
        "    c += len(word)\n",
        "    \n",
        "  url_id.append(base)\n",
        "  personal_pronouns_count.append(count_personal_pronouns(text))\n",
        "  word_avg_length.append(round(c/len(words)))"
      ],
      "metadata": {
        "id": "O1h6q8x3FC6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pronouns_and_word_length = {'url_id': url_id, 'personal_pronouns': personal_pronouns_count,\n",
        "                            'word_avg_length': word_avg_length}"
      ],
      "metadata": {
        "id": "SFdLLnQdFovM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Text_EA/result/pronouns_and_word_length.json', 'w') as file1:\n",
        "  json.dump(pronouns_and_word_length, file1)\n",
        "file1.close()"
      ],
      "metadata": {
        "id": "PW7K-D0TIPEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Collecting all the results and saving in the excel sheet."
      ],
      "metadata": {
        "id": "zxTWt7Z7MCL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_result = '/content/drive/MyDrive/Text_EA/result'"
      ],
      "metadata": {
        "id": "bmSxSm4JL9Am"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "for filename in tqdm(os.listdir(path_result)):\n",
        "  f = os.path.join(path_result, filename)\n",
        "  base, ext = os.path.splitext(filename)\n",
        "  try:\n",
        "    with open(f, 'r') as file1:\n",
        "      results[base] = json.load(file1)\n",
        "  except:\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5_apGjaQYiw",
        "outputId": "e9437b68-b205-4847-8063-a4244e1dc829"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00,  9.85it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = {}\n",
        "\n",
        "for key in results.keys():\n",
        "  print(key)\n",
        "  df[key] = pd.DataFrame.from_dict(results[key])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PF4dD1PpSHes",
        "outputId": "0d2fdf95-3ac9-4c6c-b06e-97f51cbdd954"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scores\n",
            "readability_analysis\n",
            "avg_and_complex_words\n",
            "word_and_syllable_count\n",
            "pronouns_and_word_length\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_df = pd.merge(df['scores'], df['readability_analysis'], on = 'url_id', how = 'outer')"
      ],
      "metadata": {
        "id": "3ApfNgBcUQpI"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_df1 = pd.merge(df['avg_and_complex_words'], df['word_and_syllable_count'],\n",
        "                  on = 'url_id',how = 'outer')"
      ],
      "metadata": {
        "id": "-m1VwYqWWnj9"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_df1 = pd.merge(all_df1, df['pronouns_and_word_length'], on='url_id', how = 'outer')"
      ],
      "metadata": {
        "id": "_gVgWVchWlte"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_df = pd.merge(all_df, all_df1, on = 'url_id', how = 'outer')"
      ],
      "metadata": {
        "id": "29W-eU70YAeK"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_df = all_df.astype({'url_id': float})\n",
        "all_df.sort_values('url_id', inplace=True)"
      ],
      "metadata": {
        "id": "sigR-qjrYCim"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column1 = list(all_df.columns)"
      ],
      "metadata": {
        "id": "MTBwmIf6ddyo"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing output structure sheet and saving final result."
      ],
      "metadata": {
        "id": "GWt7iKqufdo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main_df = pd.read_excel('/content/drive/MyDrive/Text_EA/Output Data Structure.xlsx')"
      ],
      "metadata": {
        "id": "qfhn91jcfNxg"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = list(main_df.columns)"
      ],
      "metadata": {
        "id": "5hxn79tcfU3d"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns.remove('URL','URL_ID')"
      ],
      "metadata": {
        "id": "q18fMB55gVLG"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_list = [column1, columns]"
      ],
      "metadata": {
        "id": "xKF1hziXgo-S"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_dict = {}\n",
        "for i in range(len(column_list[0])):\n",
        "  column_dict[column_list[0][i]] = column_list[1][i]"
      ],
      "metadata": {
        "id": "vsJinibLhknv"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_df.rename(columns=column_dict, inplace=True)"
      ],
      "metadata": {
        "id": "0bJYaHaDh4NR"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_df.drop(columns, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "KvDdg1C1kBab"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.merge(main_df, all_df, on='URL_ID', how='outer')"
      ],
      "metadata": {
        "id": "jrlY0gSmifCs"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_excel('/content/drive/MyDrive/Text_EA/result/Final_Result.xlsx')"
      ],
      "metadata": {
        "id": "x9yghx3EjCFN"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comments:\n",
        "1. The syllable per word file in the result folder is showing the exact count of syllable per in a text file. Whereas the syllable count in excel sheet is average of the text file.\n",
        "2. Three websites with URL_ID 44, 57 and 144 are broken.\n",
        "3. Final sheet is saved in the result folder."
      ],
      "metadata": {
        "id": "N8VpQh8usTFw"
      }
    }
  ]
}